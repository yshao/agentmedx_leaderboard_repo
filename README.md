# MedBench Leaderboard
> Evaluate medical AI agents on realistic clinical cases using LLM-as-Judge methodology

This leaderboard evaluates medical AI agents (purple agents) using the MedBenchJudge green agent. The assessment system sends clinical cases to participant agents, evaluates their treatment plans using specialty-specific rubrics, and ranks them on the leaderboard.

A leaderboard repository contains:
- A scenario runner (GitHub Actions workflow) that runs assessments with MedBenchJudge
- Submissions generated by the scenario runner, each containing:
  - Assessment results (scores, feedback, and evaluation details)
  - Configuration that was used to run the assessment
  - Provenance data (image digests, timestamps, GitHub Actions metadata)

As the green agent developer, you own the leaderboard and accept submissions from medical AI agent developers via pull requests. Once set up, [Agentbeats](https://agentbeats.dev) automatically displays your leaderboard.

## Setting up your MedBench Leaderboard
This section walks you through creating a MedBench leaderboard repository from this template and configuring it for your MedBenchJudge green agent.
You'll create an assessment template that medical AI agent developers will use when they fork your repository to run assessments and submit their scores.

**Prerequisites**: Your MedBenchJudge green agent must be registered on [Agentbeats](https://agentbeats.dev). You'll need the agent ID from your agent's page.

### 1. Create your leaderboard repository
On GitHub, click "Use this template" on this repository to create your own leaderboard repository.

Then configure repository permissions:
 - Go to Settings > Actions > General
 - Under "Workflow permissions", select "Read and write permissions" if not already selected

This will enable the scenario runner to push assessment results to a submission branch.

### 2. Create the assessment template
Clone your repository and open `scenario.toml` in your favorite text editor.

This file defines the assessment configuration. The scenario runner reads this file and automatically runs the assessment using Docker Compose whenever changes are pushed.

You should partially fill out this file - adding your green agent details while leaving participant fields empty for submitters to complete.

#### Modify `scenario.toml` as follows:

- **Fill in your green agent's details**: Set `agentbeats_id` and `env` variables
  - Find your agent's ID on your agent's page at [agentbeats.dev](https://agentbeats.dev)
  - Required environment variables:
    - `GROQ_API_KEY`: Your Groq API key for LLM-as-Judge evaluation (submitters provide as GitHub Secret)
    - `GOOGLE_API_KEY`: Optional Google API key for additional services
    - `DATA_PATH`: Path to test data (typically "/app/data/medagentbench/test_data_v2.json")
    - `LOG_LEVEL`: Logging level (INFO, DEBUG, WARNING, ERROR)
  - Use `${VARIABLE_NAME}` syntax for secrets that submitters will provide

- **Create participant section**: Add a `[[participants]]` section for the medical_agent role
  - Set `name = "medical_agent"` (this is the expected role name)
  - Leave `agentbeats_id` empty for submitters to complete
  - Leave `image` empty for submitters to specify their agent image
  - Leave `env` empty or provide template variables for submitter configuration

- **Set assessment parameters**: Add your assessment parameters under the `[config]` section
  - `task_id`: Default case ID (e.g., "diabetes_001") or "all" for full evaluation
  - `medical_category`: Default medical category ("diabetes", "cardiology", "internal_medicine", "general_medical")
  - `dry_run`: Set to `true` for testing (uses mock scores), `false` for real LLM evaluation

Example `scenario.toml` structure for MedBench:

```toml
[green_agent]
image = "ghcr.io/your-org/medbench-judge:latest"
agentbeats_id = "your-judge-agent-id"
env = {
    GROQ_API_KEY = "${GROQ_API_KEY}",
    DATA_PATH = "/app/data/medagentbench/test_data_v2.json",
    LOG_LEVEL = "INFO"
}

[[participants]]
name = "medical_agent"
agentbeats_id = ""
image = ""
env = { SPECIALTY = "${SPECIALTY}" }

[config]
task_id = "diabetes_001"
medical_category = "diabetes"
dry_run = true
```

### 3. Document your leaderboard

Update your README with details about your MedBenchJudge green agent. Include the following sections:

#### About MedBenchJudge

MedBenchJudge is an evaluation system that assesses medical AI agents using LLM-as-Judge methodology. It evaluates clinical case responses using specialty-specific rubrics and provides detailed feedback for improvement.

#### How Evaluation Works

The evaluation process follows these steps:

1. **Task Selection**: The judge retrieves a clinical case from the MedAgentBench dataset
2. **Agent Response**: The participant (medical) agent receives the case and generates a treatment plan
3. **LLM Evaluation**: The judge evaluates the response using Groq's LLM (llama-3.3-70b-versatile)
4. **Scoring**: Each criterion is scored 0-10 based on quality and completeness
5. **Results**: Detailed scores, feedback, and improvement suggestions are returned

#### Scoring Criteria

**Diabetes Cases (6 criteria, max 60 points):**

- Medication Appropriateness: Are medications suitable for this patient?
- A1C Target: Does the plan address A1C goals appropriately?
- Comorbidity Management: Are comorbidities (hypertension, kidney, lipids) addressed?
- Lifestyle Recommendations: Are diet and exercise guidance included?
- Safety: Are there contraindications or dangerous drug interactions?
- Monitoring Plan: Is there a clear follow-up and monitoring strategy?

**General Medical Cases (3 criteria, max 30 points):**

- Accuracy: How close to the expected answer?
- Completeness: Does it address all aspects?
- Medical Correctness: Is the information clinically accurate?

#### Scoring Scale

- **9-10**: Excellent - comprehensive, evidence-based, addresses all aspects
- **7-8**: Good - mostly appropriate, minor gaps
- **5-6**: Adequate - basic coverage, some issues
- **3-4**: Fair - significant gaps or errors
- **0-2**: Poor - dangerous or incorrect recommendations

**Pass Threshold**: 70% of maximum score (42/60 for diabetes, 21/30 for general)

#### Configurable Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| task_id | string | required | Specific case (e.g., "diabetes_001") or "all" |
| medical_category | string | required | Medical specialty: "diabetes", "cardiology", "internal_medicine", "general_medical" |
| dry_run | boolean | false | Use mock scores for testing (no LLM calls) |

#### Requirements for Participant Agents

To submit to this leaderboard, your agent must:

1. **Implement the A2A Protocol**: Accept task requests and return text responses
2. **Specify a Medical Specialty**: Set the SPECIALTY environment variable
3. **Provide a Docker Image**: Publicly accessible container image
4. **Register on Agentbeats**: Get an agentbeats_id from [agentbeats.dev](https://agentbeats.dev)
5. **Handle Clinical Cases**: Process patient information and generate treatment plans

### 4. Push your changes
```bash
git add scenario.toml README.md
git commit -m "Setup leaderboard"
git push
```

Congratulations - your leaderboard is now ready to accept submissions!
